  // interactive-local-ai.js
  // Ultimate version: history, streaming, save/load, syntax highlighting,
  // multi-line input, dynamic model switching, coding mode, clipboard

  import OpenAI from 'openai';
  import * as readline from 'node:readline/promises';
  import { stdin as input, stdout as output } from 'node:process';
  import clipboard from 'clipboardy';
  import chalk from 'chalk';
  import fs from 'fs';
  import path from 'path';
  import { fileURLToPath } from 'url';

  const __dirname = path.dirname(fileURLToPath(import.meta.url));
  const CHAT_DIR = path.join(process.env.HOME || process.env.USERPROFILE, '.local-ai-chat');
  if (!fs.existsSync(CHAT_DIR)) fs.mkdirSync(CHAT_DIR, { recursive: true });

  // â”€â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  const openai = new OpenAI({
    baseURL: 'http://localhost:11434/v1',
    apiKey: 'ollama',
  });

  let MODEL_NAME = process.env.OLLAMA_MODEL || 'gemma3';
  let currentTemperature = 0.7;
  let isCodingMode = false;

  const MAX_HISTORY = 30;

  // â”€â”€â”€ State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  let history = [
    { role: 'system', content: 'You are a helpful, clear and concise assistant.' }
  ];

  // â”€â”€â”€ Utils â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  function getChatFile(filename) {
    return path.join(CHAT_DIR, `${filename}.json`);
  }

  function saveChat(filename) {
    const file = getChatFile(filename);
    fs.writeFileSync(file, JSON.stringify(history, null, 2));
    console.log(chalk.green(`âœ“ Saved to: ${filename}.json`));
  }

  function loadChat(filename) {
    const file = getChatFile(filename);
    if (!fs.existsSync(file)) {
      console.log(chalk.red(`âœ— File not found: ${filename}`));
      return false;
    }
    history = JSON.parse(fs.readFileSync(file, 'utf8'));
    console.log(chalk.green(`âœ“ Loaded: ${filename}.json`));
    return true;
  }

  function highlightCode(text) {
    return text.replace(/```(\w+)?\n([\s\S]+?)```/g, (match, lang, code) => {
      const highlighted = chalk.cyan(code.trim());
      return `\n${chalk.bold.magenta('```' + (lang || ''))}\n${highlighted}\n${chalk.bold.magenta('```')}\n`;
    });
  }

  // â”€â”€â”€ Readline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  const rl = readline.createInterface({ input, output });

  let multiLineMode = false;
  let multiLineBuffer = [];

  async function askQuestion(prompt = 'You: ') {
    return await rl.question(prompt);
  }

  // â”€â”€â”€ Help â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  function printHelp() {
    console.log(chalk.cyan(`
  Commands:
    /clear          â†’ Clear conversation
    /save [name]    â†’ Save chat (default: conversation)
    /load [name]    â†’ Load chat
    /model <name>   â†’ Change model (e.g. /model llama3.2)
    /temp <0-2>     â†’ Change temperature
    /code           â†’ Toggle coding assistant mode
    /multi          â†’ Enter multi-line mode (type /end to send)
    /help           â†’ Show this help
    exit / bye      â†’ Exit
  `));
  }

  // â”€â”€â”€ Main Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  async function main() {
    console.log(chalk.bold.blue('â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”'));
    console.log(chalk.bold.blue('â”‚          ðŸš€ LOCAL AI CHAT â€“ Ultimate Edition         â”‚'));
    console.log(chalk.bold.blue(`â”‚  Model: ${MODEL_NAME.padEnd(45)} â”‚`));
    console.log(chalk.bold.blue('â”‚  Type /help for commands                             â”‚'));
    console.log(chalk.bold.blue('â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n'));

    console.log(chalk.yellow('Tip: Pull model if needed â†’ ') + chalk.cyan(`docker exec -it ollama-server ollama pull ${MODEL_NAME}`) + '\n');

    while (true) {
      let userInput = '';

      if (multiLineMode) {
        userInput = await askQuestion(chalk.gray('... (multi-line) '));
      } else {
        userInput = await askQuestion(chalk.bold('You: '));
      }

      userInput = userInput.trim();

      if (!userInput) continue;

      // â”€â”€â”€ Multi-line handling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      if (multiLineMode) {
        if (userInput === '/end') {
          multiLineMode = false;
          userInput = multiLineBuffer.join('\n');
          multiLineBuffer = [];
          console.log(chalk.gray('Multi-line sent.\n'));
        } else {
          multiLineBuffer.push(userInput);
          continue;
        }
      } else if (userInput === '/multi') {
        multiLineMode = true;
        console.log(chalk.cyan('Multi-line mode ON. Type /end when finished.\n'));
        continue;
      }

      const lower = userInput.toLowerCase();

      // â”€â”€â”€ Commands â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      if (['exit', 'quit', 'bye'].includes(lower)) {
        console.log(chalk.green('Goodbye! ðŸ‘‹'));
        break;
      }

      if (lower === '/clear') {
        history = [history[0]];
        console.log(chalk.green('Conversation cleared.\n'));
        continue;
      }

      if (lower === '/help') {
        printHelp();
        continue;
      }

      if (lower.startsWith('/save')) {
        const name = (userInput.split(' ')[1] || 'conversation').trim();
        saveChat(name);
        continue;
      }

      if (lower.startsWith('/load')) {
        const name = (userInput.split(' ')[1] || 'conversation').trim();
        loadChat(name);
        continue;
      }

      if (lower.startsWith('/model ')) {
        const newModel = userInput.slice(7).trim();
        if (newModel) {
          MODEL_NAME = newModel;
          console.log(chalk.green(`Model switched to: ${MODEL_NAME}\n`));
        }
        continue;
      }

      if (lower === '/code') {
        isCodingMode = !isCodingMode;
        history[0].content = isCodingMode
          ? 'You are an expert programmer. Always respond with clean, well-commented code when appropriate.'
          : 'You are a helpful, clear and concise assistant.';
        console.log(chalk.green(`Coding mode: ${isCodingMode ? 'ON' : 'OFF'}\n`));
        continue;
      }

      if (lower.startsWith('/temp ')) {
        const val = parseFloat(userInput.slice(6));
        if (!isNaN(val) && val >= 0 && val <= 2) {
          currentTemperature = val;
          console.log(chalk.green(`Temperature set to ${val}\n`));
        }
        continue;
      }

      // â”€â”€â”€ Normal chat â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      console.log('');

      try {
        history.push({ role: 'user', content: userInput });

        // Trim history
        if (history.length > MAX_HISTORY) {
          history = [history[0], ...history.slice(-MAX_HISTORY + 1)];
        }

        process.stdout.write(chalk.bold.cyan('AI : '));

        const stream = await openai.chat.completions.create({
          model: MODEL_NAME,
          messages: history,
          temperature: currentTemperature,
          max_tokens: 4096,
          stream: true,
        });

        let fullAnswer = '';

        for await (const chunk of stream) {
          const delta = chunk.choices[0]?.delta?.content || '';
          if (delta) {
            process.stdout.write(delta);
            fullAnswer += delta;
          }
        }

        console.log('\n');

        const finalAnswer = highlightCode(fullAnswer);
        console.log(finalAnswer);
        console.log(chalk.gray('â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n'));

        history.push({ role: 'assistant', content: fullAnswer });

        // Auto copy to clipboard
        try {
          await clipboard.write(fullAnswer);
        } catch {}

      } catch (err) {
        console.error(chalk.red('Error:'), err.message);
        if (err.message.includes('model') || err.message.includes('not found')) {
          console.log(chalk.yellow(`\nTip: docker exec -it ollama-server ollama pull ${MODEL_NAME}\n`));
        }
      }
    }

    rl.close();
  }

  main();